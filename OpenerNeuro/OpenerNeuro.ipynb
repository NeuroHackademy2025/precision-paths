{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code was made with the help of AI tools, including ChatGPT and GitHub Copilot.\n",
    "import pandas as pd\n",
    "from OpenerNeuro import jsons_to_dataframe, get_nii_shape_from_url, flatten_json, list_s3_json_files\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # find the latest version of a given OpenNueo datsey\n",
    "# def get_latest_snapshot(dataset_id: str):\n",
    "#     \"\"\"\n",
    "#     Fetch the latest snapshot tag for a given OpenNeuro dataset ID.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     dataset_id : str\n",
    "#         The OpenNeuro dataset ID (e.g., \"ds000001\").\n",
    "    \n",
    "#     Returns\n",
    "#     -------\n",
    "#     str\n",
    "#         The latest snapshot tag (version) for the dataset.\n",
    "\n",
    "#     \"\"\"\n",
    "#     graphql_url = \"https://openneuro.org/crn/graphql\" # GraphQL endpoint for OpenNeuro\n",
    "    \n",
    "#     # GraphQL query to fetch dataset snapshots\n",
    "#     query = \"\"\"\n",
    "#       query ($id: ID!) {\n",
    "#         dataset(id: $id) {\n",
    "#           snapshots {\n",
    "#             tag\n",
    "#           }\n",
    "#         }\n",
    "#       }\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Make the GraphQL request to fetch snapshots\n",
    "#     res = requests.post(graphql_url, json={\"query\": query, \"variables\": {\"id\": dataset_id}})\n",
    "#     res.raise_for_status() # Check for HTTP errors\n",
    "#     snaps = res.json()[\"data\"][\"dataset\"][\"snapshots\"] # Extract snapshots\n",
    "#     if not snaps: # Check if there are any snapshots\n",
    "#         raise Exception(\"No snapshots found.\")\n",
    "#     latest = sorted([s[\"tag\"] for s in snaps], reverse=True)[0] # Get the latest tag\n",
    "#     return latest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Get the urls of all JSON files for a specific version of an OpenNeuro dataset\n",
    "# def get_json_urls(dataset_id: str, version_tag: str):\n",
    "#     \"\"\"\n",
    "#     Fetch URLs of JSON files for a specific version of an OpenNeuro dataset.\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     dataset_id : str\n",
    "#         The OpenNeuro dataset ID (e.g., \"ds000001\").\n",
    "#     version_tag : str\n",
    "#         The version tag (snapshot) to fetch JSON files from.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     dict\n",
    "#         A dictionary mapping JSON filenames to their download URLs.\n",
    "#     \"\"\"\n",
    "\n",
    "#     graphql_url = \"https://openneuro.org/crn/graphql\" # GraphQL endpoint for OpenNeuro\n",
    "\n",
    "#     # GraphQL query to fetch files for a specific snapshot\n",
    "#     query = \"\"\"\n",
    "#       query ($id: ID!, $tag: String!) {\n",
    "#         snapshot(datasetId: $id, tag: $tag) {\n",
    "#             files {\n",
    "#             filename\n",
    "#             urls\n",
    "#             }\n",
    "#         }\n",
    "#     }\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Make the GraphQL request to fetch files for the specified snapshot\n",
    "#     res = requests.post(graphql_url, json={\"query\": query, \"variables\": {\"id\": dataset_id, \"tag\": version_tag}})\n",
    "#     if res.status_code != 200: # Check for HTTP errors\n",
    "#         print(\"Response content:\\n\", res.text)\n",
    "#         raise Exception(f\"GraphQL error fetching version {version_tag}: {res.status_code}\")\n",
    "#     files = res.json()[\"data\"][\"snapshot\"][\"files\"]\n",
    "    \n",
    "#     # Recursively collect all .json files\n",
    "#     def collect_json_files(directory, current_path=\"\"):\n",
    "#         jsons = {}\n",
    "#         for f in directory.get(\"files\", []):\n",
    "#             if f[\"filename\"].endswith(\".json\"):\n",
    "#                 full_path = f\"{current_path}{f['filename']}\"\n",
    "#                 jsons[full_path] = f[\"urls\"][0]\n",
    "#         for subdir in directory.get(\"directories\", []):\n",
    "#             sub_path = f\"{current_path}{subdir['name']}/\"\n",
    "#             jsons.update(collect_json_files(subdir, sub_path))\n",
    "#         return jsons\n",
    "    \n",
    "#     json_files = {f[\"filename\"]: f[\"urls\"][0] for f in files if f[\"filename\"].endswith(\".json\")}\n",
    "\n",
    "#     print(f\"Found {len(json_files)} JSON files.\")\n",
    "#     print(\"Examples:\")\n",
    "#     for example in list(json_files.keys())[:5]:\n",
    "#         print(\" --\", example)\n",
    "\n",
    "#     return json_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_json_urls(dataset_id: str, version_tag: str):\n",
    "#     \"\"\"\n",
    "#     Fetch URLs of all JSON files for a specific version of an OpenNeuro dataset.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     dataset_id : str\n",
    "#         The OpenNeuro dataset ID (e.g., \"ds000001\").\n",
    "#     version_tag : str\n",
    "#         The version tag (snapshot) to fetch JSON files from.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     dict\n",
    "#         A dictionary mapping JSON file paths (with folders) to their download URLs.\n",
    "#     \"\"\"\n",
    "\n",
    "#     graphql_url = \"https://openneuro.org/crn/graphql\"\n",
    "\n",
    "#     query = \"\"\"\n",
    "#     query ($id: ID!, $tag: String!) {\n",
    "#       snapshot(datasetId: $id, tag: $tag) {\n",
    "#         files {\n",
    "#           filename\n",
    "#           urls\n",
    "#         }\n",
    "#       }\n",
    "#     }\n",
    "#     \"\"\"\n",
    "\n",
    "#     res = requests.post(graphql_url, json={\"query\": query, \"variables\": {\"id\": dataset_id, \"tag\": version_tag}})\n",
    "#     if res.status_code != 200:\n",
    "#         print(\"Response content:\\n\", res.text)\n",
    "#         raise Exception(f\"GraphQL error fetching version {version_tag}: {res.status_code}\")\n",
    "\n",
    "#     files = res.json()[\"data\"][\"snapshot\"][\"files\"]\n",
    "\n",
    "#     # Filter for JSON files and map full path -> first url\n",
    "#     json_files = {f[\"filename\"]: f[\"urls\"][0] for f in files if f[\"filename\"].endswith(\".json\")}\n",
    "\n",
    "#     print(f\"✅ Found {len(json_files)} JSON files.\")\n",
    "#     print(\"Examples:\")\n",
    "#     for example in list(json_files.keys())[:5]:\n",
    "#         print(\"  -\", example)\n",
    "\n",
    "#     return json_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# def get_json_urls(dataset_id: str, version_tag: str):\n",
    "#     \"\"\"\n",
    "#     Fetch all JSON files (including nested) for a given dataset and version using the OpenNeuro REST API manifest.\n",
    "\n",
    "#     Returns dict mapping full path -> URL\n",
    "#     \"\"\"\n",
    "#     manifest_url = f\"https://openneuro.org/crn/datasets/{dataset_id}/snapshots/{version_tag}/contents\"\n",
    "\n",
    "#     res = requests.get(manifest_url)\n",
    "#     res.raise_for_status()\n",
    "\n",
    "#     contents = res.json()  # This is a list of file/directory dicts recursively\n",
    "\n",
    "#     json_files = {}\n",
    "\n",
    "#     def recurse_files(entries, prefix=\"\"):\n",
    "#         for entry in entries:\n",
    "#             if entry[\"type\"] == \"file\" and entry[\"name\"].endswith(\".json\"):\n",
    "#                 path = prefix + entry[\"name\"]\n",
    "#                 # The URL is in 's3Uri' or 'url' or build from key? Let's check:\n",
    "#                 # Try 'url' field if present, else construct from s3Uri\n",
    "#                 url = entry.get(\"url\")\n",
    "#                 if not url:\n",
    "#                     url = f\"https://s3.amazonaws.com/openneuro.org/{dataset_id}/{path}\"\n",
    "#                 json_files[path] = url\n",
    "#             elif entry[\"type\"] == \"directory\":\n",
    "#                 recurse_files(entry[\"contents\"], prefix + entry[\"name\"] + \"/\")\n",
    "\n",
    "#     recurse_files(contents)\n",
    "\n",
    "#     print(f\"✅ Found {len(json_files)} JSON files.\")\n",
    "#     print(\"Examples:\")\n",
    "#     for example in list(json_files.keys())[:5]:\n",
    "#         print(\" -\", example)\n",
    "\n",
    "#     return json_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import boto3\n",
    "\n",
    "# def list_s3_json_files(dataset_id):\n",
    "#     s3 = boto3.client('s3')\n",
    "#     bucket = 'openneuro.org'\n",
    "#     prefix = f\"{dataset_id}/\"\n",
    "\n",
    "#     paginator = s3.get_paginator('list_objects_v2')\n",
    "#     pages = paginator.paginate(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "#     json_files = {}\n",
    "\n",
    "#     for page in pages:\n",
    "#         for obj in page.get('Contents', []):\n",
    "#             key = obj['Key']\n",
    "#             if key.endswith('.json'):\n",
    "#                 url = f\"https://s3.amazonaws.com/{bucket}/{key}\"\n",
    "#                 json_files[key] = url\n",
    "\n",
    "#     print(f\"Found {len(json_files)} JSON files.\")\n",
    "#     for example in list(json_files.keys())[:5]:\n",
    "#         print(\" -\", example)\n",
    "\n",
    "#     return json_files\n",
    "\n",
    "# json_urls = list_s3_json_files('ds005264')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def list_s3_json_files(dataset_id):\n",
    "#     s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "#     bucket = 'openneuro.org'\n",
    "#     prefix = f\"{dataset_id}/\"\n",
    "\n",
    "#     paginator = s3.get_paginator('list_objects_v2')\n",
    "#     pages = paginator.paginate(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "#     json_files = {}\n",
    "\n",
    "#     for page in pages:\n",
    "#         for obj in page.get('Contents', []):\n",
    "#             key = obj['Key']\n",
    "#             if key.endswith('.json'):\n",
    "#                 url = f\"https://s3.amazonaws.com/{bucket}/{key}\"\n",
    "#                 json_files[key] = url\n",
    "\n",
    "#     print(f\"Found {len(json_files)} JSON files.\")\n",
    "#     for example in list(json_files.keys())[:10]:\n",
    "#         print(\" -\", example)\n",
    "\n",
    "#     return json_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # flatten json objects into a single-level dictionary\n",
    "# def flatten_json(y):\n",
    "#     \"\"\"\n",
    "#     Flatten a nested JSON object into a single-level dictionary with dot notation keys.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     y : dict\n",
    "#         The nested JSON object to flatten.\n",
    "    \n",
    "#     Returns\n",
    "#     -------\n",
    "#     dict\n",
    "#         A flattened dictionary with keys in dot notation.\n",
    "#     \"\"\"\n",
    "\n",
    "#     out = {} # Initialize output dictionary\n",
    "\n",
    "#     # Recursive function to flatten the JSON\n",
    "#     def _flatten(x, name=\"\"):\n",
    "#         if isinstance(x, dict):\n",
    "#             for k,v in x.items():\n",
    "#                 _flatten(v, name + k + \".\")\n",
    "#         elif isinstance(x, list):\n",
    "#             for i,v in enumerate(x):\n",
    "#                 _flatten(v, name + str(i) + \".\")\n",
    "#         else:\n",
    "#             out[name[:-1]] = x\n",
    "#     _flatten(y)\n",
    "\n",
    "#     return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Load JSON files from an OpenNeuro dataset URL into a Pandas DataFrame\n",
    "# def jsons_to_dataframe(openneuro_url: str):\n",
    "#     \"\"\"\n",
    "#     Load JSON files from an OpenNeuro dataset URL into a Pandas DataFrame.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     openneuro_url : str\n",
    "#         The OpenNeuro dataset URL (e.g., \"https://openneuro.org/datasets/ds000001/versions/1.0.0\").\n",
    "    \n",
    "#     Returns\n",
    "#     -------\n",
    "#     pd.DataFrame\n",
    "#         A Pandas DataFrame containing the flattened JSON data, indexed by file name.\n",
    "#     \"\"\"\n",
    "\n",
    "#     parsed = urlparse(openneuro_url)\n",
    "#     parts = parsed.path.strip(\"/\").split(\"/\")\n",
    "#     dataset_id = parts[1]\n",
    "#     version = parts[3] if len(parts) > 3 and parts[2] == \"versions\" else None\n",
    "\n",
    "#     if version is None:\n",
    "#         version = get_latest_snapshot(dataset_id) #######\n",
    "#         print(f\"No version specified — using latest: {version}\")\n",
    "#     else:\n",
    "#         print(f\"Using specified version: {version}\")\n",
    "\n",
    "#     json_urls = get_json_urls(dataset_id, version) #######\n",
    "#     print(f\"Found {len(json_urls)} JSON files for version {version}\")\n",
    "\n",
    "#     records = []\n",
    "#     for fname, url in tqdm(json_urls.items(), desc=\"Downloading JSONs\"):\n",
    "#         r = requests.get(url)\n",
    "#         r.raise_for_status()\n",
    "#         flat = flatten_json(r.json()) #######\n",
    "#         flat[\"__file__\"] = fname\n",
    "#         records.append(flat)\n",
    "\n",
    "#     df = pd.DataFrame(records).set_index(\"__file__\")\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def jsons_to_dataframe(openneuro_url: str):\n",
    "#     \"\"\"\n",
    "#     Load JSON files from an OpenNeuro dataset URL into a Pandas DataFrame.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     openneuro_url : str\n",
    "#         The OpenNeuro dataset URL (e.g., \"https://openneuro.org/datasets/ds000001/versions/1.0.0\").\n",
    "    \n",
    "#     Returns\n",
    "#     -------\n",
    "#     pd.DataFrame\n",
    "#         A Pandas DataFrame containing the flattened JSON data, indexed by file name.\n",
    "#     \"\"\"\n",
    "\n",
    "#     parsed = urlparse(openneuro_url)\n",
    "#     parts = parsed.path.strip(\"/\").split(\"/\")\n",
    "#     dataset_id = parts[1]\n",
    "#     # version is not used for S3 listing, so no need to parse it here\n",
    "\n",
    "#     print(f\"Listing JSON files for dataset: {dataset_id} from S3 (no versioning)\")\n",
    "\n",
    "#     # Call your S3 JSON file lister WITHOUT version arg\n",
    "#     json_urls = list_s3_json_files(dataset_id)  # <-- Removed version argument here\n",
    "\n",
    "#     print(f\"Found {len(json_urls)} JSON files.\")\n",
    "\n",
    "#     records = []\n",
    "#     for fname, url in tqdm(json_urls.items(), desc=\"Downloading JSONs\"):\n",
    "#         r = requests.get(url)\n",
    "#         r.raise_for_status()\n",
    "#         flat = flatten_json(r.json())\n",
    "#         flat[\"__file__\"] = fname\n",
    "#         records.append(flat)\n",
    "\n",
    "#     df = pd.DataFrame(records).set_index(\"__file__\")\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_json_urls(\"ds005264\", \"1.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# json_urls = list_s3_json_files('ds005264')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = jsons_to_dataframe(\"https://openneuro.org/datasets/ds005264\")\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def get_scan_duration(df):\n",
    "#     \"\"\"\n",
    "#     Calculate the duration of each scan in seconds from combinations of the 'AcquisitionDuration','SeriesTime','AcquisitionTime', and 'RepetitionTime' fields in the DataFrame.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     df : pd.DataFrame\n",
    "#         The DataFrame containing flattened JSON data with a 'duration' field.\n",
    "    \n",
    "#     Returns\n",
    "#     -------\n",
    "#     pd.Series\n",
    "#         A Pandas Series containing the scan durations in seconds.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # check if dataframe has any of the required columns\n",
    "#     required_columns = ['AcquisitionDuration', 'SeriesTime', 'AcquisitionTime', 'RepetitionTime', 'NumberOfVolumes']\n",
    "#     if not any(col in df.columns for col in required_columns):\n",
    "#         raise ValueError(f\"DataFrame lacks all required columns: {required_columns}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = jsons_to_dataframe(\"https://openneuro.org/datasets/ds005118\")\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 'AcquisitionDuration', 'SeriesTime', 'AcquisitionTime', 'RepetitionTime', 'NumberOfVolumes'\n",
    "# df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def list_s3_niigz_files(dataset_id):\n",
    "#     \"\"\"\n",
    "#     List all .nii.gz files in a given OpenNeuro dataset (via public S3).\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     dataset_id : str\n",
    "#         The OpenNeuro dataset ID (e.g., \"ds000001\").\n",
    "    \n",
    "#     Returns\n",
    "#     -------\n",
    "#     dict\n",
    "#         A dictionary mapping file paths to their download URLs.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "#     bucket = 'openneuro.org'\n",
    "#     prefix = f\"{dataset_id}/\"\n",
    "\n",
    "#     paginator = s3.get_paginator('list_objects_v2')\n",
    "#     pages = paginator.paginate(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "#     niigz_files = {}\n",
    "\n",
    "#     for page in pages:\n",
    "#         for obj in page.get('Contents', []):\n",
    "#             key = obj['Key']\n",
    "#             if key.endswith('.nii.gz'):\n",
    "#                 url = f\"https://s3.amazonaws.com/{bucket}/{key}\"\n",
    "#                 niigz_files[key] = url\n",
    "\n",
    "#     print(f\"Found {len(niigz_files)} NIfTI files (.nii.gz).\")\n",
    "#     for example in list(niigz_files.keys())[:10]:\n",
    "#         print(\" -\", example)\n",
    "\n",
    "#     return niigz_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nii_files = list_s3_niigz_files(\"ds003814\")\n",
    "# nii_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_nii_shape_from_url(url):\n",
    "#     \"\"\"\n",
    "#     Fetch a NIfTI file from a URL, decompress it if gzipped, and return its shape.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     url : str\n",
    "#         The s3 URL of the NIfTI file (can be gzipped).\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     tuple\n",
    "#         The shape of the NIfTI image (dimensions).\n",
    "#     \"\"\"\n",
    "#     response = requests.get(url)\n",
    "#     response.raise_for_status()\n",
    "\n",
    "#     # Decompress gzip content\n",
    "#     with gzip.GzipFile(fileobj=io.BytesIO(response.content)) as gz:\n",
    "#         decompressed = gz.read()\n",
    "\n",
    "#     # Use FileHolder with decompressed bytes for header and image\n",
    "#     file_holder = nib.FileHolder(fileobj=io.BytesIO(decompressed))\n",
    "#     img = nib.Nifti1Image.from_file_map({'header': file_holder, 'image': file_holder})\n",
    "\n",
    "#     return img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://s3.amazonaws.com/openneuro.org/ds005118/sub-ME01/ses-func01/func/sub-ME01_ses-func01_task-rest_run-01_echo-01_bold.nii.gz\"\n",
    "# shape = get_nii_shape_from_url(url)\n",
    "# print(\"Shape:\", shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds005118/sub-ME01/ses-func01/func/sub-ME01_ses-func01_task-rest_run-01_echo-01_bold.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_json_metadata_from_openneuro_file(dataset_id: str, relative_path=: str):\n",
    "#     \"\"\"\n",
    "#     Fetch and flatten a single JSON file from the OpenNeuro S3 archive.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     dataset_id : str\n",
    "#         The OpenNeuro dataset ID, e.g. \"ds003814\"\n",
    "\n",
    "#     relative_path : str\n",
    "#         The full relative path to the JSON file within the dataset,\n",
    "#         e.g. \"sub-ident01/func/sub-ident01_task-rest_bold.json\"\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     dict\n",
    "#         A flattened dictionary of the JSON contents.\n",
    "#     \"\"\"\n",
    "#     # Build the S3 URL\n",
    "#     url = f\"https://s3.amazonaws.com/openneuro.org/{dataset_id}/{relitive_path}\"\n",
    "\n",
    "#     # Download and parse JSON\n",
    "#     r = requests.get(url)\n",
    "#     r.raise_for_status()\n",
    "#     raw_json = r.json()\n",
    "\n",
    "#     # Flatten the JSON\n",
    "#     return flatten_json(raw_json)\n",
    "\n",
    "# def get_json_metadata_from_openneuro_file(dataset_id, relative_path):\n",
    "#     bucket = \"openneuro.org\"\n",
    "#     url = f\"https://s3.amazonaws.com/{bucket}/{dataset_id}/{relative_path}\"\n",
    "#     response = requests.get(url)\n",
    "#     response.raise_for_status()\n",
    "#     return response.json()\n",
    "\n",
    "# def get_scan_time_from_TR(json_path: str, TR: float = None, num_volumes: int = None,):\n",
    "#     \"\"\"\n",
    "#     Calculate the total scan time in seconds based on TR and number of volumes from a JSON file.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     json_path : str\n",
    "#         The path to the JSON file containing scan metadata.\n",
    "#     TR : float\n",
    "#         The repetition time (TR) in seconds.\n",
    "#     num_volumes : int\n",
    "#         The number of volumes in the scan. If not provided, it will be inferred from the NIfTI file shape.\n",
    "    \n",
    "#     Returns\n",
    "#     -------\n",
    "#     float\n",
    "#         The total scan time in seconds.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     if TR is None:\n",
    "#         with open(json_path, 'r') as f:\n",
    "#             metadata = flatten_json(pd.read_json(f).to_dict(orient='records')[0])\n",
    "#             TR = metadata.get('RepetitionTime', None)\n",
    "#             if TR is None:\n",
    "#                 raise ValueError(\"JSON file does not contain 'RepetitionTime' field.\")\n",
    "\n",
    "#         if num_volumes is None:\n",
    "#             num_volumes = metadata.get('NumberOfVolumes', None)\n",
    "#             if num_volumes is None:\n",
    "#                 shape = get_nii_shape_from_url(url)\n",
    "#                 if len(shape) < 4:\n",
    "#                     raise ValueError(\"NIfTI file does not have a time dimension (4th dimension).\")\n",
    "#                 num_volumes = shape[3]\n",
    "\n",
    "#     return num_volumes * TR\n",
    "\n",
    "# def convert_hhmmss_string_to_seconds(time_str: str):\n",
    "#     \"\"\"\n",
    "#     Convert a time string in HH:MM:SS format to seconds.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     time_str : str\n",
    "#         The time string in HH:MM:SS format.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     float   \n",
    "#         The time in seconds.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     if not isinstance(time_str, str):\n",
    "#         raise ValueError(\"Time string must be a string in HH:MM:SS format.\")\n",
    "    \n",
    "#     parts = time_str.split(':')\n",
    "#     if len(parts) != 3:\n",
    "#         raise ValueError(\"Time string must be in HH:MM:SS format.\")\n",
    "\n",
    "#     hours, minutes, seconds = map(float, parts)\n",
    "#     return hours * 3600 + minutes * 60 + seconds\n",
    "\n",
    "# def get_scan_time_from_AcquisitionTime_SeriesTime(json_path: str, AcquisitionTime: str = None, SeriesTime: str = None):\n",
    "#     \"\"\"\n",
    "#     Calculate the scan duration in seconds based on AcquisitionTime and SeriesTime from a JSON file.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     json_path : str\n",
    "#         The path to the JSON file containing scan metadata.\n",
    "#     AcquisitionTime : str\n",
    "#         The acquisition time in HH:MM:SS format.\n",
    "#     SeriesTime : str\n",
    "#         The series time in HH:MM:SS format.\n",
    "    \n",
    "#     Returns\n",
    "#     -------\n",
    "#     float\n",
    "#         The scan duration in seconds.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     with open(json_path, 'r') as f:\n",
    "#         metadata = flatten_json(pd.read_json(f).to_dict(orient='records')[0])\n",
    "    \n",
    "#     if AcquisitionTime is None:\n",
    "#         AcquisitionTime = metadata.get('AcquisitionTime', None)\n",
    "#         if AcquisitionTime is None:\n",
    "#             raise ValueError(\"JSON file does not contain 'AcquisitionTime' field.\")\n",
    "    \n",
    "#     if SeriesTime is None:\n",
    "#         SeriesTime = metadata.get('SeriesTime', None)\n",
    "#         if SeriesTime is None:\n",
    "#             raise ValueError(\"JSON file does not contain 'SeriesTime' field.\")\n",
    "        \n",
    "#     # Check that output is not negative\n",
    "#     output = convert_hhmmss_string_to_seconds(AcquisitionTime) - convert_hhmmss_string_to_seconds(SeriesTime)\n",
    "#     if output < 0:\n",
    "#         raise ValueError(\"AcquisitionTime cannot be earlier than SeriesTime.\")\n",
    "    \n",
    "#     return output\n",
    "    \n",
    "# def get_scan_time(json_path: str, TR: float = None, AcquisitionTime: str = None,\n",
    "#                   SeriesTime: str = None, num_volumes: int = None,\n",
    "#                   AcquisitionDuration: float = None):\n",
    "#     \"\"\" \n",
    "#     Calculate the scan duration in seconds using available metadata.\n",
    "\n",
    "#     Priority: AcquisitionDuration > AcquisitionTime & SeriesTime > TR & num_volumes\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     json_path : str\n",
    "#         Path to the JSON file with scan metadata.\n",
    "#     TR : float, optional\n",
    "#         Repetition time (in seconds).\n",
    "#     AcquisitionTime : str, optional\n",
    "#         Acquisition time (HH:MM:SS).\n",
    "#     SeriesTime : str, optional\n",
    "#         Series time (HH:MM:SS).\n",
    "#     num_volumes : int, optional\n",
    "#         Number of volumes.\n",
    "#     AcquisitionDuration : float, optional\n",
    "#         Known scan duration in seconds.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     float\n",
    "#         Scan duration in seconds.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Step 1: Use provided AcquisitionDuration directly\n",
    "#     if AcquisitionDuration is not None:\n",
    "#         return AcquisitionDuration\n",
    "    \n",
    "#     else: # Step 1.5: Try to read JSON and get AcquisitionDuration from metadata\n",
    "#         try:\n",
    "#             metadata = get_json_metadata_from_openneuro_file(dataset_id, json_path)\n",
    "#         except Exception as e:\n",
    "#             raise ValueError(f\"Could not retrieve JSON metadata: {e}\")\n",
    "#         try:\n",
    "#             if 'AcquisitionDuration' in metadata:\n",
    "#                 return metadata['AcquisitionDuration']\n",
    "#             else:\n",
    "#                 raise ValueError(\"JSON file does not contain 'AcquisitionDuration' field.\")\n",
    "#         except ValueError as e:\n",
    "#             print(f\"Warning: Failed to extract 'AcquisitionDuration' from JSON: {e}\")\n",
    "\n",
    "#         # Step 2: Try using AcquisitionTime and SeriesTime\n",
    "#         try:\n",
    "#             scan_time = get_scan_time_from_AcquisitionTime_SeriesTime(json_path, AcquisitionTime, SeriesTime)\n",
    "#             if scan_time is not None:\n",
    "#                 return scan_time\n",
    "#         except ValueError as e:\n",
    "#             print(f\"Warning: Failed to compute scan time from AcquisitionTime/SeriesTime: {e}\")\n",
    "\n",
    "#         # Step 3: Try using TR and number of volumes\n",
    "#         try:\n",
    "#             scan_time = get_scan_time_from_TR(json_path, TR, num_volumes)\n",
    "#             if scan_time is not None:\n",
    "#                 return scan_time\n",
    "#         except ValueError as e:\n",
    "#             print(f\"Warning: Failed to compute scan time from TR and num_volumes: {e}\")\n",
    "\n",
    "#     # Step 5: If all methods fail, raise an error\n",
    "#     raise ValueError(\"Unable to determine scan duration from any available metadata.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def get_json_metadata_from_openneuro_file(dataset_id: str, relative_path: str):\n",
    "    \"\"\"\n",
    "    Fetch and parse a JSON file from the OpenNeuro S3 archive.\n",
    "\n",
    "    Returns dict (unflattened).\n",
    "    \"\"\"\n",
    "    url = f\"https://s3.amazonaws.com/openneuro.org/{dataset_id}/{relative_path}\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def get_nii_shape_from_url(url):\n",
    "    \"\"\"\n",
    "    Fetch a NIfTI file from a URL, decompress it if gzipped, and return its shape.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        The s3 URL of the NIfTI file (can be gzipped).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        The shape of the NIfTI image (dimensions).\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Decompress gzip content\n",
    "    with gzip.GzipFile(fileobj=io.BytesIO(response.content)) as gz:\n",
    "        decompressed = gz.read()\n",
    "\n",
    "    # Use FileHolder with decompressed bytes for header and image\n",
    "    file_holder = nib.FileHolder(fileobj=io.BytesIO(decompressed))\n",
    "    img = nib.Nifti1Image.from_file_map({'header': file_holder, 'image': file_holder})\n",
    "\n",
    "    return img.shape\n",
    "\n",
    "def get_scan_time_from_TR(dataset_id: str, relative_json_path: str, TR: float = None, num_volumes: int = None):\n",
    "    \"\"\"\n",
    "    Calculate total scan time using TR and number of volumes.\n",
    "    Fetches JSON from OpenNeuro S3 if needed.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_id : str\n",
    "        OpenNeuro dataset ID, e.g., \"ds003814\"\n",
    "    relative_json_path : str\n",
    "        Relative path to the JSON file within the dataset.\n",
    "    TR : float, optional\n",
    "        Repetition time (seconds), overrides JSON if provided.\n",
    "    num_volumes : int, optional\n",
    "        Number of volumes, overrides JSON if provided.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Total scan time in seconds.\n",
    "    \"\"\"\n",
    "    metadata = get_json_metadata_from_openneuro_file(dataset_id, relative_json_path)\n",
    "    metadata = flatten_json(metadata)\n",
    "\n",
    "    if TR is None:\n",
    "        TR = metadata.get('RepetitionTime', None)\n",
    "        if TR is None:\n",
    "            raise ValueError(\"JSON metadata does not contain 'RepetitionTime' field.\")\n",
    "\n",
    "    if num_volumes is None:\n",
    "        num_volumes = metadata.get('NumberOfVolumes', None)\n",
    "\n",
    "        if num_volumes is None:\n",
    "            # Optional: try to infer from NIfTI shape\n",
    "            nifti_path = relative_json_path.replace('.json','.nii.gz')\n",
    "            print(f\"Inferring number of volumes from NIfTI file: {nifti_path}\")\n",
    "            nifti_url = f\"https://s3.amazonaws.com/openneuro.org/{dataset_id}/{nifti_path}\"\n",
    "            shape = get_nii_shape_from_url(nifti_url)\n",
    "\n",
    "            if len(shape) < 4:\n",
    "                raise ValueError(\"NIfTI file does not have a time dimension (4th dimension).\")\n",
    "\n",
    "            num_volumes = shape[3]\n",
    "\n",
    "    return num_volumes * TR\n",
    "\n",
    "def convert_hhmmss_string_to_seconds(time_str: str):\n",
    "    if not isinstance(time_str, str):\n",
    "        raise ValueError(\"Time string must be a string in HH:MM:SS format.\")\n",
    "    parts = time_str.split(':')\n",
    "    if len(parts) != 3:\n",
    "        raise ValueError(\"Time string must be in HH:MM:SS format.\")\n",
    "    hours, minutes, seconds = map(float, parts)\n",
    "    return hours * 3600 + minutes * 60 + seconds\n",
    "\n",
    "def get_scan_time_from_AcquisitionTime_SeriesTime(metadata: dict, AcquisitionTime: str = None, SeriesTime: str = None):\n",
    "    \"\"\"\n",
    "    Compute scan time using AcquisitionTime and SeriesTime from metadata.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    metadata : dict\n",
    "        The metadata dictionary containing 'AcquisitionTime' and 'SeriesTime'.\n",
    "    AcquisitionTime : str, optional\n",
    "        The acquisition time in HH:MM:SS format. If None, will be fetched from metadata.\n",
    "    SeriesTime : str, optional\n",
    "        The series time in HH:MM:SS format. If None, will be fetched from metadata.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The scan duration in seconds.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If AcquisitionTime or SeriesTime is not provided and not found in metadata,\n",
    "        or if AcquisitionTime is earlier than SeriesTime.\n",
    "    \"\"\"\n",
    "    if AcquisitionTime is None:\n",
    "        AcquisitionTime = metadata.get('AcquisitionTime', None)\n",
    "        if AcquisitionTime is None:\n",
    "            raise ValueError(\"Metadata does not contain 'AcquisitionTime' field.\")\n",
    "    \n",
    "    if SeriesTime is None:\n",
    "        SeriesTime = metadata.get('SeriesTime', None)\n",
    "        if SeriesTime is None:\n",
    "            raise ValueError(\"Metadata does not contain 'SeriesTime' field.\")\n",
    "    \n",
    "    output = convert_hhmmss_string_to_seconds(AcquisitionTime) - convert_hhmmss_string_to_seconds(SeriesTime)\n",
    "    if output < 0:\n",
    "        raise ValueError(\"AcquisitionTime cannot be earlier than SeriesTime.\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def get_scan_time(dataset_id: str, json_path: str, TR: float = None, AcquisitionTime: str = None,\n",
    "                  SeriesTime: str = None, num_volumes: int = None,\n",
    "                  AcquisitionDuration: float = None):\n",
    "    \"\"\"\n",
    "    Calculate scan duration using metadata from JSON and parameters.\n",
    "    Priority: AcquisitionDuration > AcquisitionTime/SeriesTime > TR & num_volumes\n",
    "    \"\"\"\n",
    "    if AcquisitionDuration is not None:\n",
    "        return AcquisitionDuration\n",
    "\n",
    "    # Attempt to fetch metadata from OpenNeuro\n",
    "    try:\n",
    "        metadata = get_json_metadata_from_openneuro_file(dataset_id, json_path)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Could not retrieve JSON metadata: {e}\")\n",
    "\n",
    "    # Use AcquisitionDuration if present in metadata\n",
    "    if 'AcquisitionDuration' in metadata:\n",
    "        return metadata['AcquisitionDuration']\n",
    "\n",
    "    # Try AcquisitionTime and SeriesTime\n",
    "    try:\n",
    "        scan_time = get_scan_time_from_AcquisitionTime_SeriesTime(json_path, AcquisitionTime, SeriesTime)\n",
    "        if scan_time is not None:\n",
    "            return scan_time\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to compute scan time from AcquisitionTime/SeriesTime: {e}\")\n",
    "\n",
    "    # Try TR and num_volumes\n",
    "    try:\n",
    "        scan_time = get_scan_time_from_TR(json_path, TR, num_volumes)\n",
    "        if scan_time is not None:\n",
    "            return scan_time\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to compute scan time from TR and num_volumes: {e}\")\n",
    "\n",
    "    raise ValueError(\"Unable to determine scan duration from any available metadata.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'AcquisitionDuration', 'SeriesTime', 'AcquisitionTime', 'RepetitionTime', 'NumberOfVolumes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_s3_json_files('ds003814')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://s3.amazonaws.com/openneuro.org/ds003814/sub-ident01/func/sub-ident01_task-rest_run-01_echo-1_bold.json\n",
    "\n",
    "# metadata = get_json_metadata_from_openneuro_file(\n",
    "#     dataset_id=\"ds003814\",\n",
    "#     json_path=\"sub-ident01/func/sub-ident01_task-rest_run-01_echo-1_bold.json\"\n",
    "# )\n",
    "\n",
    "# # View flattened keys and values\n",
    "# for k, v in metadata.items():\n",
    "#     print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_id = \"ds003814\"\n",
    "# json_path = \"sub-ident01/func/sub-ident01_task-rest_run-01_echo-1_bold.json\"\n",
    "\n",
    "# get_json_metadata_from_openneuro_file(dataset_id=dataset_id, relative_path=json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_id = \"ds003814\"\n",
    "# json_path = \"sub-ident01/func/sub-ident01_task-rest_run-01_echo-1_bold.json\"\n",
    "\n",
    "# try:\n",
    "#     duration = get_scan_time(json_path=json_path)\n",
    "#     print(f\"Scan duration (seconds): {duration}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Failed to get scan duration: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_path = \"sub-ident01/func/sub-ident01_task-rest_run-01_echo-1_bold.json\"\n",
    "\n",
    "# try:\n",
    "#     duration = get_scan_time(json_path=json_path)\n",
    "#     print(f\"Scan duration (seconds): {duration}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Failed to get scan duration: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to compute scan time from AcquisitionTime/SeriesTime: 'str' object has no attribute 'get'\n",
      "Warning: Failed to compute scan time from TR and num_volumes: 404 Client Error: Not Found for url: https://s3.amazonaws.com/openneuro.org/sub-ident01/func/sub-ident01_task-rest_run-01_echo-1_bold.json/None\n",
      "Failed to get scan duration: Unable to determine scan duration from any available metadata.\n"
     ]
    }
   ],
   "source": [
    "dataset_id = \"ds003814\"\n",
    "json_path = \"sub-ident01/func/sub-ident01_task-rest_run-01_echo-1_bold.json\"\n",
    "\n",
    "try:\n",
    "    duration_seconds = get_scan_time(dataset_id=dataset_id, json_path=json_path)\n",
    "    print(f\"Scan duration (seconds): {duration_seconds}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to get scan duration: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "everything",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
